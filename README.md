# Affective-TRM : Reconnaissance d'√âmotion Multimodale R√©cursive

![Python](https://img.shields.io/badge/Python-3.10%2B-blue) ![PyTorch](https://img.shields.io/badge/PyTorch-2.0%2B-red) ![Status](https://img.shields.io/badge/Status-Research_Prototype-purple)

**Affective-TRM** est une architecture exp√©rimentale de Deep Learning con√ßue pour la reconnaissance d'√©motions en continu (Valence / Arousal) √† partir de flux vid√©o. 

Contrairement aux approches classiques qui classifient une √©motion en cat√©gories discr√®tes (ex: "Col√®re", "Joie"), ce mod√®le projette l'√©tat √©motionnel dans un **espace latent continu** en fusionnant trois modalit√©s : **Audio, Vid√©o et Texte**.

## üß† Architecture du Mod√®le

Le c≈ìur du projet repose sur le **Tiny Recursive Reasoning Model (TRM)**. C'est une architecture hybride qui combine la puissance des Transformers avec l'efficacit√© s√©quentielle des RNNs.

### Points Cl√©s de l'Architecture :
1.  **Entr√©e Multimodale Massive (2816 dims)** :
    *   **Audio (768)** : Features extraites via Wav2Vec/Hubert.
    *   **Vid√©o (1280)** : Features spatiales issues d'EfficientNet (enet_b0).
    *   **Texte (768)** : Embeddings s√©mantiques issus de LLM (Llama/Gemma).
    *   *Fusion* : Les modalit√©s sont concat√©n√©es et normalis√©es (LayerNorm) avant d'entrer dans le r√©seau.

2.  **Transformer R√©current (TRM)** :
    *   Au lieu de traiter toute la vid√©o d'un coup (ce qui exploserait la VRAM), le mod√®le traite la s√©quence frame par frame.
    *   Il maintient une **m√©moire persistante (Carry State)** compos√©e de :
        *   $z_H$ (High-level) : Contexte global et √©motionnel √† long terme.
        *   $z_L$ (Low-level) : M√©moire de travail pour les calculs imm√©diats.
    *   √Ä chaque pas de temps, l'input est inject√© et fusionn√© avec la m√©moire via des blocs d'attention (SwiGLU + RoPE).

3.  **Dual-Path Decision** :
    *   **Shortcut Head** : Une voie rapide qui permet au mod√®le de r√©agir aux signaux √©vidents (ex: un cri fort) imm√©diatement.
    *   **Deep Reasoning Head** : Une voie profonde qui analyse le contexte temporel stock√© dans $z_H$ pour affiner la pr√©diction.

## üî¨ M√©thodologie et Donn√©es

Ce projet utilise une m√©thodologie de transformation de donn√©es innovante pour convertir un dataset de classification (ex: CREMA-D) en probl√®me de r√©gression.

### 1. Mapping Discret vers Continu
Les √©motions discr√®tes sont mapp√©es sur l'espace Valence/Arousal (Mod√®le Circumplex de Russell) :
*   **Col√®re (ANG)** $\rightarrow$ Valence N√©gative / Arousal Haut
*   **Tristesse (SAD)** $\rightarrow$ Valence N√©gative / Arousal Bas
*   **Joie (HAP)** $\rightarrow$ Valence Positive / Arousal Haut
*   *Etc.*

### 2. Gestion de l'Intensit√© & Data Augmentation
Pour √©viter que le mod√®le n'apprenne des points fixes par c≈ìur, nous utilisons une strat√©gie de **Label Smoothing Spatial** :
*   Chaque √©motion poss√®de un centre de gravit√© th√©orique.
*   Ce centre est d√©plac√© selon l'intensit√© annot√©e (`LO`, `MD`, `HI`).
*   Pour les intensit√©s inconnues (`XX`), une intensit√© al√©atoire est simul√©e.
*   Un **bruit gaussien** est ajout√© √† chaque √©chantillon.
*   **R√©sultat :** Le mod√®le doit apprendre √† viser des "zones" √©motionnelles plut√¥t que des coordonn√©es exactes, ce qui am√©liore consid√©rablement la g√©n√©ralisation.

### 3. Fonction de Perte (Loss) Hybride
L'entra√Ænement minimise une combinaison de deux pertes :
$$Loss = (1 - CCC) + \alpha \times MSE_{zone}$$
*   **CCC (Concordance Correlation Coefficient)** : Maximise la corr√©lation temporelle et l'accord d'amplitude.
*   **Zone Loss (MSE)** : Guide le mod√®le vers le bon quadrant √©motionnel, crucial en d√©but d'entra√Ænement.

## üöÄ Installation et Utilisation

### Pr√©-requis
*   Python 3.10+
*   PyTorch avec support CUDA
*   `uv` (recommand√©) ou `pip`

### 1. Pr√©paration des Donn√©es
Le script de pr√©paration scanne les fichiers bruts, extrait les embeddings (Audio/Vid√©o/Texte) et g√©n√®re un dataset `.pt` optimis√©.

```bash
# V√©rifiez les chemins dans src/config.py avant de lancer
uv run prepare_data.py
```
*Note : Cette √©tape peut √™tre longue car elle effectue l'inf√©rence des encodeurs (Audio/Vid√©o/LLM).*

### 2. Entra√Ænement
Lance la boucle d'entra√Ænement avec validation crois√©e (Speaker Independent Split).

```bash
uv run train.py
```
Le script g√®re automatiquement :
*   La normalisation des entr√©es.
*   Le split Train/Val/Test (garantissant qu'un acteur n'est pas vu en train et en test).
*   La sauvegarde du meilleur mod√®le.
*   L'affichage des courbes de Loss et CCC.

### 3. Visualisation
√Ä la fin de l'entra√Ænement, deux graphiques sont g√©n√©r√©s :
1.  **Historique d'apprentissage** : √âvolution de la Loss et du score CCC.
2.  **Espace Valence/Arousal** : Un scatter plot montrant les pr√©dictions (rouge) vs la v√©rit√© terrain (bleu), permettant d'analyser la dynamique du mod√®le (ex: ph√©nom√®ne de r√©gression vers la moyenne).

## üìä R√©sultats Observ√©s

Sur le dataset CREMA-D transform√© :
*   **CCC Score** : ~0.77 (Performance √©tat de l'art pour cette approche).
*   **Comportement** : Le mod√®le d√©montre une capacit√© robuste √† distinguer les valences positives/n√©gatives. Il adopte un comportement conservateur sur l'intensit√© (r√©gression vers la moyenne), typique des approches par r√©gression sur des donn√©es bruit√©es.

## üìÇ Structure du Projet

```
.
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ config.py           # Configuration globale (Hyperparam√®tres, Chemins)
‚îÇ   ‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ dataset.py      # Gestion du Dataset PyTorch & Collate
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ preprocessor.py # Extraction des features & Mapping √âmotionnel
‚îÇ   ‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ trm.py          # Architecture Tiny Recursive Model
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ layers.py       # Blocs de base (Attention, RMSNorm, SwiGLU)
‚îÇ   ‚îî‚îÄ‚îÄ training/
‚îÇ       ‚îú‚îÄ‚îÄ engine.py       # Boucle d'entra√Ænement & Fonctions de Loss
‚îÇ       ‚îî‚îÄ‚îÄ visualizer.py   # Outils de plotting (Matplotlib)
‚îú‚îÄ‚îÄ train.py                # Point d'entr√©e principal
‚îú‚îÄ‚îÄ prepare_data.py         # Script de pr√©-traitement
‚îî‚îÄ‚îÄ README.md
```

## üìú Cr√©dits
*   **Architecture TRM** : Inspir√©e des travaux sur les *Recurrent Transformers* et *Adaptive Computation Time*.
*   **Dataset** : Bas√© sur CREMA-D (Crowd-sourced Emotional Multimodal Actors Dataset).
*   **Encoders** : Utilise des poids pr√©-entra√Æn√©s pour l'extraction de features (Wav2Vec2, EfficientNet, Gemma/Llama).