# Affective-TRM : Reconnaissance d'√âmotion Multimodale R√©cursive

![Python](https://img.shields.io/badge/Python-3.11%2B-blue) ![PyTorch](https://img.shields.io/badge/PyTorch-2.8%2B-red) ![Status](https://img.shields.io/badge/Status-Research_Prototype-purple) ![HuggingFace](https://img.shields.io/badge/Model-HuggingFace-yellow)

**Affective-TRM** est une architecture de Deep Learning con√ßue pour la reconnaissance d'√©motions en continu (**Valence / Arousal**) √† partir de flux vid√©o temps r√©el.

Contrairement aux approches classiques qui classifient une √©motion en cat√©gories discr√®tes (ex: "Col√®re", "Joie"), ce mod√®le projette l'√©tat √©motionnel dans un **espace latent continu** en fusionnant trois modalit√©s : **Audio, Vid√©o et Texte**.

Le mod√®le est capable de tourner en temps r√©el avec une empreinte m√©moire r√©duite gr√¢ce √† son architecture r√©cursive.

---

## üß† Architecture du Mod√®le

Le c≈ìur du projet repose sur le **Tiny Recursive Reasoning Model (TRM)**. C'est une architecture hybride inspir√©e des travaux de Samsung SAIL, combinant la puissance des Transformers avec l'efficacit√© s√©quentielle des RNNs.

<p align="center">
  <img src="docs/arch.png" alt="Architecture TRM" width="300"/>
</p>


*Architecture du Tiny Recursive Model (Source: Samsung SAIL)*

### 1. Entr√©e Multimodale Massive (2816 dims)
Le mod√®le ing√®re un vecteur concat√©n√© repr√©sentant l'√©tat complet de l'utilisateur √† un instant $t$ :
*   **Audio (768 dims)** : Features extraites via **Dasheng** (Wav2Vec/Hubert optimis√©).
*   **Vid√©o (1280 dims)** : Features spatiales issues d'**EfficientNet-B0** pr√©-entra√Æn√© sur AFEW (via HSEmotion).
*   **Texte (768 dims)** : Embeddings s√©mantiques issus de **EmbeddingGemma** (Google).
*   *Fusion* : Les modalit√©s sont concat√©n√©es horizontalement et normalis√©es (LayerNorm).

### 2. Le Flux de Donn√©es (Pipeline)
![Pipeline](docs/pipeline.png)

*   **Synchronisation** : Le syst√®me est cadenc√© sur la vid√©o (30 FPS).
*   **Audio** : Alignement temporel par fen√™tres glissantes synchronis√©es avec les frames.
*   **Texte** : Injection continue du contexte s√©mantique (dupliqu√© temporellement).
*   **R√©currence** : √Ä chaque pas de temps, le TRM met √† jour sa m√©moire latente $Z$ en fonction de l'entr√©e $X_t$ et de son √©tat pr√©c√©dent $Z_{t-1}$.

---

## üî¨ M√©thodologie et Entra√Ænement

Ce projet transforme un dataset de classification (CREMA-D) en probl√®me de r√©gression continue.

### 1. Mapping Discret $\rightarrow$ Continu
Les √©motions discr√®tes sont projet√©es sur le mod√®le Circumplex de Russell via des centro√Ødes, avec ajout de **bruit gaussien** et modulation d'intensit√© pour simuler une distribution r√©elle :
*   **Col√®re** $\rightarrow$ Valence N√©gative / Arousal Haut
*   **Joie** $\rightarrow$ Valence Positive / Arousal Haut
*   **Tristesse** $\rightarrow$ Valence N√©gative / Arousal Bas
*   *Etc.*

### 2. Fonction de Perte (Loss) Hybride
Pour garantir √† la fois la compr√©hension de la dynamique √©motionnelle et la stabilit√© des pr√©dictions, nous utilisons une Loss combin√©e :

$$Loss_{total} = Loss_{CCC} + 0.25 \times MSE_{real}$$

*   **$Loss_{CCC}$ ($1 - CCC$)** : Maximise la corr√©lation de forme. Force le mod√®le √† comprendre les variations (mont√©e/descente) de l'√©motion.
*   **$MSE_{real}$ (Mean Squared Error)** : Agit comme une ancre pour emp√™cher les pr√©dictions de d√©river hors de l'espace $[-1, 1]$ et maintient l'√©chelle correcte.

---

## üìä R√©sultats

Le mod√®le a √©t√© entra√Æn√© sur 25 √©poques avec une s√©paration stricte par locuteur (Speaker Independent).

| M√©trique | Score Final (Ep 25) | Interpr√©tation |
| :--- | :--- | :--- |
| **Train Loss** | 0.804 | Bonne convergence de l'apprentissage. |
| **Val Loss** | 0.839 | Pas d'overfitting majeur (courbe stable). |
| **Val CCC** | **0.255** | Corr√©lation positive significative sur des donn√©es synth√©tiques bruit√©es. |

### Visualisations
| Historique d'Apprentissage | Espace Valence/Arousal (Test) |
| :---: | :---: |
| ![History](docs/training_history.png) | ![AV Space](docs/av_space_plot.png) |
| *La dynamique du CCC (courbe verte) montre une am√©lioration constante jusqu'√† la fin.* | *Le mod√®le (rouge) couvre correctement les quadrants haut d√©finis par la v√©rit√© terrain (bleu), mais se retrouve en difficult√© pour la partie en bas √† droite.* |


## ‚ö†Ô∏è Limitations et Observations (Post-Mortem)

Bien que l'architecture technique (TRM + Fusion Multimodale) soit fonctionnelle sur CPU, des biais ont √©t√© observ√©s lors de l'inf√©rence en conditions r√©elles (Webcam) :

1.  **Biais "Bas-Gauche" (Low Arousal / Negative Valence)** :
    *   Le mod√®le tend √† √™tre conservateur et pr√©dit majoritairement des √©tats neutres ou l√©g√®rement n√©gatifs.
    *   Il peine √† atteindre le quadrant "Haut-Droit" (Joie/Excitation) sans une exag√©ration volontaire de l'utilisateur.
    *   **Cause probable** : L'utilisation d'une **phrase neutre mise en cache** lors de l'inf√©rence (pour √©conomiser le CPU) prive le mod√®le de la modalit√© s√©mantique positive. De plus, le dataset d'entra√Ænement (CREMA-D) est jou√© par des acteurs, cr√©ant un d√©calage (Domain Shift) avec des expressions naturelles devant une webcam.

## üîÆ Pistes d'Am√©lioration (Roadmap)

Ce projet est une preuve de concept (PoC). Pour passer √† un syst√®me robuste en production, voici les axes prioritaires :

### 1. Donn√©es
*   Abandonner le mapping artificiel (Classification $\to$ R√©gression).
*   Utiliser des datasets annot√©s nativement en Valence/Arousal continu, tels que **RECOLA**, **SEWA** ou **MSP-IMPROV**. Cela permettrait au mod√®le d'apprendre de vraies nuances humaines plut√¥t que des centro√Ødes simul√©s.

### 2. Gestion du Texte (Alignement Temporel)
*   Actuellement : Le texte est dupliqu√© globalement sur toute la s√©quence.
*   Cible : Impl√©menter un **alignement mot-√†-mot** (Word-Level Alignment). Le vecteur s√©mantique ne devrait changer que lorsque le mot est prononc√©.
*   Int√©gration d'un ASR l√©ger (ex: **Vosk** ou **Whisper-Tiny**) pour g√©n√©rer le texte en temps r√©el au lieu d'utiliser une phrase d'ancrage neutre.

### 3. Architecture
*   Remplacer la concat√©nation simple par un m√©canisme de **Cross-Attention** (le texte "interroge" la vid√©o) pour mieux pond√©rer l'importance de chaque modalit√© selon le contexte (ex: ignorer la vid√©o s'il fait sombre, ignorer l'audio s'il y a du bruit).




---

## üöÄ Installation et Utilisation

### Pr√©-requis
*   Python 3.11+
*   PyTorch (CUDA recommand√©)
*   `uv` ou `pip`

### 1. T√©l√©chargement des D√©pendances
Ce script t√©l√©charge automatiquement le dataset, les poids des encodeurs (Llama, Gemma, ENet) et les outils n√©cessaires.
```bash
python downloads.py
```

### 2. Pr√©paration des Donn√©es
Extrait les features Audio/Vid√©o/Texte et cr√©e le dataset `.pt`.
```bash
python prepare_data.py
```

### 3. Entra√Ænement (Optionnel)
Si vous souhaitez r√©-entra√Æner le mod√®le depuis z√©ro :
```bash
python train.py
```

### 4. Inf√©rence Temps R√©el
Lance la webcam, enregistre l'audio et affiche l'√©motion en direct.
```bash
python run_inference.py
```

---

## üì• Mod√®le Pr√©-entra√Æn√©

Les poids du mod√®le entra√Æn√© (TRM) sont disponibles sur HuggingFace :
ü§ó **[HuggingFace: JusteLeo/AffectiveTRM](https://huggingface.co/JusteLeo/AffectiveTRM)**

T√©l√©chargez `emotion_model_sequential_av.pth` et placez-le √† la racine du projet si vous ne voulez pas lancer l'entra√Ænement.

---

## üìú Cr√©dits et R√©f√©rences

Ce projet n'aurait pas √©t√© possible sans les travaux de recherche et les mod√®les open-source suivants :

*   **Architecture TRM** : Bas√© sur *Tiny Recursive Models* (Samsung SAIL Montreal).
    *   [GitHub Repository](https://github.com/SamsungSAILMontreal/TinyRecursiveModels)
*   **Encodeur Audio** : *Dasheng* (DCASE 2023 Winner).
    *   [GitHub Repository](https://github.com/RicherMans/Dasheng)
*   **Encodeur Vid√©o** : *HSEmotion* (EfficientNet-B0 sur AFEW).
    *   [GitHub Repository](https://github.com/av-savchenko/hsemotion)
*   **Encodeur Texte** : *EmbeddingGemma* (Google).
    *   [HuggingFace Model](https://huggingface.co/google/embeddinggemma-300m)
*   **Dataset** : *CREMA-D* (Crowd-sourced Emotional Multimodal Actors Dataset).
    *   [GitHub Repository](https://github.com/CheyneyComputerScience/CREMA-D)
---

*Projet r√©alis√© dans le cadre d'un Master 1 √âlectronique / IA.*